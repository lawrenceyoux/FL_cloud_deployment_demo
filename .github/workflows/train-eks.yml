name: Train Baseline on EKS

# Manually triggered — build the FL Docker image, push to ECR, and run
# the Phase 3 baseline training Job on EKS.  Results are logged to the
# MLflow server already running in the cluster (deployed by k8s-setup.yml).
#
# Prerequisites (run once first):
#   1. "Terraform — FL Infrastructure"  (apply)  — provisions EKS, S3, ECR repo
#   2. "K8s — Cluster Setup & Add-ons"           — deploys MLflow to EKS
#
# Workflow flow:
#   build-push  ──►  run-on-eks
#       │                 │
#   Docker image    K8s Job (train_baseline.py)
#   pushed to ECR   logs streamed here
#                   metrics → MLflow on EKS

on:
  workflow_dispatch:
    inputs:
      epochs:
        description: "Training epochs"
        required: false
        default: "50"
      model:
        description: "Model architecture"
        required: false
        default: "embeddings"
        type: choice
        options:
          - simple
          - embeddings
      loss:
        description: "Loss function"
        required: false
        default: "weighted_bce"
        type: choice
        options:
          - weighted_bce
          - focal
          - bce
      skip_job:
        description: "Build & push image only — do not run EKS Job"
        required: false
        default: "false"
        type: choice
        options:
          - "false"
          - "true"

permissions:
  contents: read

env:
  ECR_REPO:     fl-stroke
  CLUSTER_NAME: fl-demo-cluster
  NAMESPACE:    federated-learning

# ── Job 1: Build Docker image and push to ECR ─────────────────────────────────
jobs:
  build-push:
    name: "Build & push Docker image"
    runs-on: ubuntu-latest
    outputs:
      image_uri: ${{ steps.ecr.outputs.image_uri }}
      run_id:    ${{ steps.run_id.outputs.run_id }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Derive short run ID (git SHA prefix)
        id: run_id
        run: echo "run_id=$(echo $GITHUB_SHA | cut -c1-7)" >> "$GITHUB_OUTPUT"

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id:     ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region:            ${{ secrets.AWS_REGION }}

      - name: Create ECR repository (idempotent)
        run: |
          aws ecr describe-repositories --repository-names "$ECR_REPO" \
            --region "${{ secrets.AWS_REGION }}" > /dev/null 2>&1 \
          || aws ecr create-repository \
               --repository-name "$ECR_REPO" \
               --region "${{ secrets.AWS_REGION }}" \
               --image-scanning-configuration scanOnPush=true

      - name: Compute image URI
        id: ecr
        run: |
          set -e

          ACCOUNT=$(aws sts get-caller-identity --query Account --output text)
          if [ -z "$ACCOUNT" ]; then
            echo "❌ Failed to resolve AWS account ID — check AWS_ACCESS_KEY_ID / AWS_SECRET_ACCESS_KEY secrets"
            exit 1
          fi

          REGION="${{ secrets.AWS_REGION }}"
          if [ -z "$REGION" ]; then
            echo "❌ AWS_REGION secret is not set — add it in Repo → Settings → Secrets → Actions"
            exit 1
          fi

          REGISTRY="${ACCOUNT}.dkr.ecr.${REGION}.amazonaws.com"
          IMAGE_URI="${REGISTRY}/${ECR_REPO}:${{ steps.run_id.outputs.run_id }}"

          echo "image_uri=${IMAGE_URI}" >> "$GITHUB_OUTPUT"
          echo "registry=${REGISTRY}"  >> "$GITHUB_OUTPUT"
          echo "Image URI: ${IMAGE_URI}"

      - name: Debug — verify image_uri output
        run: |
          echo "──── build-push step outputs ─────────────────────────────"
          echo "steps.ecr.outputs.image_uri : '${{ steps.ecr.outputs.image_uri }}'"
          echo "steps.ecr.outputs.registry  : '${{ steps.ecr.outputs.registry }}'"
          echo "steps.run_id.outputs.run_id : '${{ steps.run_id.outputs.run_id }}'"
          echo "GITHUB_SHA (raw)            : '${GITHUB_SHA}'"
          echo "secrets.AWS_REGION set?     : '${{ secrets.AWS_REGION != '' && 'YES' || 'NO (EMPTY SECRET)' }}'"
          echo "──────────────────────────────────────────────────────────"
          if [ -z "${{ steps.ecr.outputs.image_uri }}" ]; then
            echo "❌ image_uri is empty — Compute image URI step did not write to GITHUB_OUTPUT"
            exit 1
          fi
          echo "✅ image_uri looks good, proceeding to docker login"

      - name: Login to Amazon ECR
        run: |
          aws ecr get-login-password --region "${{ secrets.AWS_REGION }}" | \
            docker login --username AWS --password-stdin "${{ steps.ecr.outputs.registry }}"
          echo "✅ Docker logged in to ECR"

      - name: Build Docker image
        run: |
          docker build \
            --file docker/Dockerfile \
            --tag "${{ steps.ecr.outputs.image_uri }}" \
            --tag "${{ steps.ecr.outputs.image_uri }}-latest" \
            .

      - name: Push to ECR
        run: |
          docker push "${{ steps.ecr.outputs.image_uri }}"
          echo "✅ Pushed: ${{ steps.ecr.outputs.image_uri }}"

# ── Job 2: Run training Job on EKS ────────────────────────────────────────────
  run-on-eks:
    name: "Run training Job on EKS"
    runs-on: ubuntu-latest
    needs: build-push
    if: github.event.inputs.skip_job != 'true'

    env:
      # FL_IMAGE is NOT passed as a cross-job output because
      # aws-actions/configure-aws-credentials@v4 masks the AWS account ID via
      # core.setSecret(). Any job output whose value contains the masked account
      # ID is blanked entirely by GitHub Actions before it reaches the next job.
      # Instead we recompute the URI from RUN_ID (which crosses safely — it is
      # a plain git SHA with no masked content) after AWS creds are configured.
      RUN_ID:       ${{ needs.build-push.outputs.run_id }}
      TRAIN_EPOCHS: ${{ github.event.inputs.epochs }}
      TRAIN_MODEL:  ${{ github.event.inputs.model }}
      TRAIN_LOSS:   ${{ github.event.inputs.loss }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Debug — inspect values received from build-push
        run: |
          echo "──── run-on-eks received values ──────────────────────────"
          echo "RUN_ID       : '${RUN_ID}'"
          echo "TRAIN_EPOCHS : '${TRAIN_EPOCHS}'"
          echo "TRAIN_MODEL  : '${TRAIN_MODEL}'"
          echo "TRAIN_LOSS   : '${TRAIN_LOSS}'"
          echo "──────────────────────────────────────────────────────────"
          MISSING=""
          [ -z "${RUN_ID}" ]       && MISSING="${MISSING} RUN_ID"
          [ -z "${TRAIN_EPOCHS}" ] && MISSING="${MISSING} TRAIN_EPOCHS"
          [ -z "${TRAIN_MODEL}" ]  && MISSING="${MISSING} TRAIN_MODEL"
          [ -z "${TRAIN_LOSS}" ]   && MISSING="${MISSING} TRAIN_LOSS"
          if [ -n "${MISSING}" ]; then
            echo "❌ The following env vars are empty:${MISSING}"
            exit 1
          fi
          echo "✅ RUN_ID and TRAIN_* are present"

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id:     ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region:            ${{ secrets.AWS_REGION }}

      - name: Compute FL_IMAGE from RUN_ID
        # Reconstruct the ECR image URI locally. We cannot use the build-push
        # job output because configure-aws-credentials masks the account ID,
        # which causes GitHub Actions to blank any cross-job output that
        # contains it. RUN_ID (plain git SHA) crosses safely; the account ID
        # is resolved fresh here from the configured credentials.
        run: |
          ACCOUNT=$(aws sts get-caller-identity --query Account --output text)
          REGION="${{ secrets.AWS_REGION }}"
          FL_IMAGE="${ACCOUNT}.dkr.ecr.${REGION}.amazonaws.com/${ECR_REPO}:${RUN_ID}"
          echo "FL_IMAGE=${FL_IMAGE}" >> "$GITHUB_ENV"
          echo "✅ FL_IMAGE = ${FL_IMAGE}"

      - name: Update kubeconfig
        run: |
          aws eks update-kubeconfig \
            --region "${{ secrets.AWS_REGION }}" \
            --name  "$CLUSTER_NAME"

      - name: Grant caller cluster-admin access
        run: |
          CALLER_ARN=$(aws sts get-caller-identity --query Arn --output text)
          aws eks create-access-entry \
            --cluster-name "$CLUSTER_NAME" \
            --principal-arn "$CALLER_ARN" \
            --region "${{ secrets.AWS_REGION }}" 2>/dev/null || true
          aws eks associate-access-policy \
            --cluster-name "$CLUSTER_NAME" \
            --principal-arn "$CALLER_ARN" \
            --policy-arn arn:aws:eks::aws:cluster-access-policy/AmazonEKSClusterAdminPolicy \
            --access-scope type=cluster \
            --region "${{ secrets.AWS_REGION }}" 2>/dev/null || true

      - name: Check MLflow is running
        run: |
          echo "Checking MLflow server in mlops namespace…"
          kubectl get pods -n mlops -l app=mlflow-server
          STATUS=$(kubectl get pods -n mlops -l app=mlflow-server \
                     -o jsonpath='{.items[0].status.phase}' 2>/dev/null || echo "NotFound")
          if [ "$STATUS" != "Running" ]; then
            echo "⚠️  MLflow pod is not Running (status=$STATUS)."
            echo "Run the 'K8s — Cluster Setup & Add-ons' workflow first."
            exit 1
          fi
          echo "✅ MLflow is Running"

      - name: Ensure mlflow-sa ServiceAccount exists
        # Idempotent: apply the SA from the versioned manifest so the cluster
        # always matches the repo. This also sets any IRSA annotations declared
        # in fl-namespace.yaml without requiring a separate manual step.
        run: |
          kubectl apply -f kubernetes/namespaces/fl-namespace.yaml
          echo "✅ Namespace + mlflow-sa ServiceAccount applied (idempotent)"

      - name: Delete previous Job with same run-id (if any)
        run: |
          kubectl delete job "train-baseline-${RUN_ID}" \
            -n "$NAMESPACE" --ignore-not-found=true

      - name: Apply training Job  (envsubst replaces FL_IMAGE / TRAIN_* / RUN_ID)
        run: |
          # ubuntu-latest ships gettext-base, but install gracefully just in case
          sudo apt-get install -y gettext-base -q 2>/dev/null || true

          # Fail fast: if the build-push job did not export FL_IMAGE the Job
          # spec will have an empty image field and Kubernetes will reject it
          # with "Required value" — surface the real cause here instead.
          if [ -z "${FL_IMAGE}" ]; then
            echo "❌ FL_IMAGE is empty. The build-push job did not export image_uri."
            echo "   Check the 'Login to Amazon ECR' step in the build-push job."
            exit 1
          fi
          if [ -z "${RUN_ID}" ]; then
            echo "❌ RUN_ID is empty."
            exit 1
          fi

          echo "FL_IMAGE = ${FL_IMAGE}"
          echo "RUN_ID   = ${RUN_ID}"
          echo "EPOCHS   = ${TRAIN_EPOCHS}  MODEL = ${TRAIN_MODEL}  LOSS = ${TRAIN_LOSS}"
          echo ""

          # Pass an explicit variable list so envsubst only replaces the five
          # known placeholders and does not clobber any unrelated ${...} tokens
          # that might appear in the YAML (e.g. shell variable expressions).
          RENDERED=$(envsubst '${FL_IMAGE} ${RUN_ID} ${TRAIN_EPOCHS} ${TRAIN_MODEL} ${TRAIN_LOSS}' \
                       < kubernetes/jobs/train-baseline.yaml)

          echo "──── Rendered manifest ────────────────────────────────────────"
          echo "${RENDERED}"
          echo "───────────────────────────────────────────────────────────────"

          echo "${RENDERED}" | kubectl apply -f -
          EXIT_CODE=$?
          if [ $EXIT_CODE -ne 0 ]; then
            echo "❌ kubectl apply failed with exit code $EXIT_CODE"
            echo "   Check the rendered manifest above for syntax errors."
            exit 1
          fi
          echo ""
          echo "✅ Job resource submitted: train-baseline-${RUN_ID}"

      - name: Verify Job was created
        run: |
          echo "═══ Checking if Job resource exists ══════════════════════════"
          kubectl get job "train-baseline-${RUN_ID}" -n "$NAMESPACE" -o yaml || {
            echo "❌ Job resource not found in cluster — apply may have silently failed"
            exit 1
          }
          echo ""
          echo "═══ Job conditions ════════════════════════════════════════════"
          kubectl get job "train-baseline-${RUN_ID}" -n "$NAMESPACE" -o jsonpath='{.status.conditions}' | jq '.' 2>/dev/null || echo "(no conditions yet)"
          echo ""

      - name: Stream training logs
        run: |
          echo "═══ Waiting for pod to appear ════════════════════════════════"
          JOB_NAME="train-baseline-${RUN_ID}"
          # Wait up to 3 min for at least one pod to be created
          for i in $(seq 1 18); do
            POD_COUNT=$(kubectl get pods -l "job-name=${JOB_NAME}" -n "$NAMESPACE" --no-headers 2>/dev/null | wc -l)
            if [ "$POD_COUNT" -gt 0 ]; then
              echo "✅ Pod(s) found after $((i * 10))s"
              break
            fi
            echo "  ⏳ No pods yet… (${i}/18, waiting 10s)"
            sleep 10
          done

          # If still zero pods after 3 minutes, dump diagnostics
          POD_COUNT=$(kubectl get pods -l "job-name=${JOB_NAME}" -n "$NAMESPACE" --no-headers 2>/dev/null | wc -l)
          if [ "$POD_COUNT" -eq 0 ]; then
            echo ""
            echo "❌❌❌ NO PODS CREATED AFTER 3 MINUTES ❌❌❌"
            echo ""
            echo "═══ Job describe (why no pods?) ═══════════════════════════════"
            kubectl describe job "train-baseline-${RUN_ID}" -n "$NAMESPACE" || true
            echo ""
            echo "═══ Recent events in namespace $NAMESPACE ═════════════════════"
            kubectl get events -n "$NAMESPACE" --sort-by='.lastTimestamp' | tail -30 || true
            echo ""
            echo "═══ Check ServiceAccount / RBAC ═══════════════════════════════"
            kubectl get serviceaccount -n "$NAMESPACE" || true
            echo ""
            echo "═══ Node status (schedulable?) ════════════════════════════════"
            kubectl get nodes -o wide || true
            echo ""
            echo "Possible causes:"
            echo "  • Image pull secret missing → add imagePullSecrets to Job spec"
            echo "  • ServiceAccount doesn't exist → create 'fl-trainer' SA"
            echo "  • No schedulable nodes → check EKS node group"
            echo "  • Resource limits too high → reduce memory/CPU requests"
            echo "══════════════════════════════════════════════════════════════"
            exit 1
          fi

          echo ""
          echo "═══ Pod list ═════════════════════════════════════════════════"
          kubectl get pods -l "job-name=${JOB_NAME}" -n "$NAMESPACE" -o wide || true

          echo ""
          echo "═══ Pod describe (events / image pull errors) ════════════════"
          kubectl describe pods -l "job-name=${JOB_NAME}" -n "$NAMESPACE" || true

          echo ""
          echo "═══ Training logs (streaming) ════════════════════════════════"
          kubectl logs \
            --selector="job-name=${JOB_NAME}" \
            --namespace="$NAMESPACE" \
            --follow \
            --pod-running-timeout=180s \
          || true
          echo "═══ End of streamed logs ═════════════════════════════════════"

      - name: Wait for Job completion and check exit code
        run: |
          echo "Waiting for Job to complete or fail (timeout: 60 min)…"
          # NOTE: never write:  (kubectl wait --for=complete ...) || (kubectl wait --for=failed ...) && exit 1
          # Shell evaluates as: (A || B) && C — if A succeeds, C (exit 1) still runs.
          # Use an explicit if/else so success and failure paths are unambiguous.
          if kubectl wait \
               --for=condition=complete \
               --timeout=3600s \
               job/"train-baseline-${RUN_ID}" \
               -n "$NAMESPACE"; then
            echo "✅ Job completed successfully"
          else
            echo "❌ Job did not complete — checking for failure condition…"
            kubectl wait \
              --for=condition=failed \
              --timeout=30s \
              job/"train-baseline-${RUN_ID}" \
              -n "$NAMESPACE" || true
            echo "❌ Job failed — see logs above"
            exit 1
          fi

      - name: Show final job status
        if: always()
        run: |
          JOB_NAME="train-baseline-${RUN_ID}"
          echo "=== Job status ==="
          kubectl describe job "train-baseline-${RUN_ID}" -n "$NAMESPACE" || true
          echo ""
          echo "=== Pod exit / conditions ==="
          kubectl get pods -l "job-name=${JOB_NAME}" -n "$NAMESPACE" -o wide || true
          echo ""
          echo "══════════════════════════════════════════════════════════════"
          echo "  MLflow UI — view results with:"
          echo "    kubectl port-forward -n mlops svc/mlflow-server 5000:5000"
          echo "    open http://localhost:5000"
          echo "  Experiment: stroke-prediction-baseline"
          echo "══════════════════════════════════════════════════════════════"
